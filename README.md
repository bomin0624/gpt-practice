# gpt-practice

bi-gram language model practice from Andrej Karpathy's lesson [Let's build GPT: from scratch, in code, spelled out.](https://youtu.be/kCc8FmEb1nY?si=ZWtKkkUKP_QRwLSM)

Model Architecture
---

```
Context length: 32 characters
Embedding dim: 64
Attention heads: 4
Transformer layers: 4
Parameters: 0.209729 M
```

Implements multi-head self-attention, feed-forward networks, layer normalization, and residual connections.


Training Result
---
<img width="1150" height="400" alt="image" src="https://github.com/user-attachments/assets/3fb3286a-63cb-4e54-8c0d-e188b3b4923b" />


References:
---

[Attention is all you need.](https://arxiv.org/pdf/1706.03762)
